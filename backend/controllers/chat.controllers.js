import { Pinecone } from '@pinecone-database/pinecone';
import { HuggingFaceTransformersEmbeddings } from "@langchain/community/embeddings/huggingface_transformers";
import { PineconeStore } from "@langchain/community/vectorstores/pinecone";
import { ChatGoogleGenerativeAI } from "@langchain/google-genai";
import { PromptTemplate } from "@langchain/core/prompts";
import { RunnableSequence } from "@langchain/core/runnables"; 
import { StringOutputParser } from "@langchain/core/output_parsers";
import { RecursiveCharacterTextSplitter } from "@langchain/textsplitters";
import { Document } from "@langchain/core/documents";

let vectorStore; 


const cleanVietnameseText = (text) => {
    return text
      .replace(/Dƒ© nhi√™n r·∫±ng|H√¨nh dung r·∫±ng l√†|√†|·ª´m/g, '') 
      .replace(/\s+/g, ' ') 
      .trim();
  };
  
  // Chunking theo tokens (ph√π h·ª£p ti·∫øng Vi·ªát)
  const createTextSplitter = () => 
    new RecursiveCharacterTextSplitter({
      chunkSize: 500, 
      chunkOverlap: 50, 
      separators: ['\n\n', '\n', '. ', '! ', '? '], 
    });

export const initializeChatbotComponents = async () => {

    const pinecone = new Pinecone({
        apiKey: process.env.PINECONE_API_KEY,
    }, process.env.PINECONE_ENVIRONMENT);


    const pineconeIndex = pinecone.Index(process.env.PINECONE_INDEX_NAME);


    const embeddings = new HuggingFaceTransformersEmbeddings({
        modelName: "intfloat/multilingual-e5-small"
    });


    vectorStore = await PineconeStore.fromExistingIndex(embeddings, {
        pineconeIndex,
    });


    const apiKeyGemini = process.env.GEMINI_API_KEY;
    const model = new ChatGoogleGenerativeAI({
        apiKey: apiKeyGemini,
        modelName: "gemini-1.5-pro",
    });

    return { vectorStore, embeddings, model, pineconeIndex };
};

export const indexLessonDataWithLangchain = async (lesson, embeddings, pineconeIndex) => {
    if (!lesson?._id || !lesson.summary || !lesson.transcript) return;
  

    const cleanedSummary = cleanVietnameseText(lesson.summary);
    const cleanedTranscript = cleanVietnameseText(lesson.transcript);
  

    const textSplitter = createTextSplitter();
    

    const transcriptChunks = await textSplitter.splitText(cleanedTranscript);
    

    const summaryChunks = await textSplitter.splitText(cleanedSummary);
  

    const allChunks = [...transcriptChunks, ...summaryChunks];
    const docs = allChunks.map((chunk, index) => 
      new Document({
        pageContent: chunk,
        metadata: { 
          lessonId: lesson._id,
          sourceType: index < transcriptChunks.length ? 'transcript' : 'summary', 
        },
      })
    );
  

    try {
      await PineconeStore.fromDocuments(docs, embeddings, { 
        pineconeIndex,
        namespace: 'lessons', 
      });
      console.log(`‚úÖ ƒê√£ index ${docs.length} chunks c·ªßa b√†i ${lesson._id} (Model: ${embeddings.modelName})`);
    } catch (error) {
      console.error("‚ùå L·ªói index:", error);
      throw new Error("Index failed: " + error.message);
    }
  };

  export const handleUserMessage = async (userQuestion, currentLessonData,currentLessonId, vectorStore, model) => {
    if (!userQuestion?.trim()) throw new Error("C√¢u h·ªèi kh√¥ng ƒë∆∞·ª£c ƒë·ªÉ tr·ªëng.");
  
    try {
  
      // B∆∞·ªõc 1: T√¨m ki·∫øm trong Pinecone (n·∫øu c·∫ßn)
      const pineconeResults = await vectorStore.similaritySearch(userQuestion, 3);
      const pineconeContext = pineconeResults.map(doc => doc.pageContent).join("\n");
  
      // B∆∞·ªõc 2: K·∫øt h·ª£p context t·ª´ b√†i h·ªçc + Pinecone
      const combinedContext = `
        TH√îNG TIN B√ÄI H·ªåC ${currentLessonId} HI·ªÜN T·∫†I:
        ${currentLessonData}
  
        TH√îNG TIN LI√äN QUAN T·ª™ H·ªÜ TH·ªêNG:
        ${pineconeContext}
      `;
  
      // B∆∞·ªõc 3: T·∫°o prompt v√† g·ªçi model
      const promptTemplate = PromptTemplate.fromTemplate(`
        B·∫°n ƒëang xem b√†i h·ªçc v·ªÅ **"React"**. H√£y tr·∫£ l·ªùi c√¢u h·ªèi sau d·ª±a v√†o n·ªôi dung d∆∞·ªõi ƒë√¢y:
        ------
        ${currentLessonData}
        ------
        C√¢u h·ªèi: ${userQuestion}

        N·∫øu th√¥ng tin tr√™n kh√¥ng ƒë·ªß, tham kh·∫£o th√™m:
        ------
        ${pineconeContext}
        ------
        Y√™u c·∫ßu:
        - ∆Øu ti√™n th√¥ng tin t·ª´ "TH√îNG TIN B√ÄI H·ªåC HI·ªÜN T·∫†I".
        - N·∫øu kh√¥ng ƒë·ªß th√¥ng tin, d√πng d·ªØ li·ªáu t·ª´ h·ªá th·ªëng.
        - Lu√¥n tr·∫£ l·ªùi b·∫±ng ti·∫øng Vi·ªát, c√≥ icon c·∫£m x√∫c.
        - Kh√¥ng ƒë·ªÅ c·∫≠p ƒë·∫øn n·ªôi dung ngo√†i React hay li√™n quan t·ªõi l·∫≠p tr√¨nh n·∫øu ng∆∞·ªùi d√πng h·ªèi v·ªÅ m·ªôt n·ªôi dung n√†o ƒë√≥ g·∫ßn g·∫ßn li√™n quan t·ªõi react c·ª© vi·ªác tr·∫£ l·ªùi nh·∫±m cho ng∆∞·ªùi d√πng hi·ªÉu r√µ h∆°n , lu√¥n k√®m icon ph√π h·ª£p üöÄ
      `);
  
      const chain = RunnableSequence.from([
        { context: () => combinedContext, question: (input) => input.question },
        promptTemplate,
        model,
        new StringOutputParser(),
      ]);
  
      return await chain.invoke({ question: userQuestion });
    } catch (error) {
      console.error("[RAG Error]:", error);
      throw new Error(`X·ª≠ l√Ω th·∫•t b·∫°i: ${error.message}`);
    }
  };


  // export const handleUserMessage = async (userQuestion, vectorStore, embeddings, model) => { 
  //   if (!userQuestion?.trim()) {
  //     throw new Error("C√¢u h·ªèi kh√¥ng ƒë∆∞·ª£c ƒë·ªÉ tr·ªëng.");
  //   }
  
  //   try {
  //     if (!vectorStore) {
  //       throw new Error("Vector store ch∆∞a ƒë∆∞·ª£c kh·ªüi t·∫°o.");
  //     }
  
  //     // B∆∞·ªõc 1: T√¨m ki·∫øm c√≥ l·ªçc metadata (vd: lessonId n·∫øu c·∫ßn)
  //     const results = await vectorStore.similaritySearch(userQuestion, 5, {
  //       // Th√™m filter n·∫øu c·∫ßn (vd: ch·ªâ t√¨m trong transcript)
  //       // filter: { sourceType: "transcript" } 
  //     });
  
  //     // B∆∞·ªõc 2: Gi·ªõi h·∫°n ƒë·ªô d√†i context ƒë·ªÉ tr√°nh v∆∞·ª£t token limit
  //     const MAX_CONTEXT_TOKENS = 3000; // T√πy model (vd: GPT-3.5 ~ 4096 tokens)
  //     let contextText = "";
  //     for (const doc of results) {
  //       const docContent = `[Ngu·ªìn: ${doc.metadata.sourceType}]\n${doc.pageContent}\n---\n`;
  //       if ((contextText + docContent).length > MAX_CONTEXT_TOKENS) break;
  //       contextText += docContent;
  //     }
  
  //     // B∆∞·ªõc 3: T·ªëi ∆∞u prompt ƒë·ªÉ gi·∫£m hallucination
  //     const promptTemplate = PromptTemplate.fromTemplate(`
  //       B·∫°n l√† m·ªôt chuy√™n gia React.js. H√ÉY TR·∫¢ L·ªúI D·ª∞A TR√äN CONTEXT SAU, KH√îNG T·ª∞ B·ªäA ƒê√ÅP √ÅN:
  //       ------
  //       {context}
  //       ------
  //       C√¢u h·ªèi: {question}
  
  //       Y√™u c·∫ßu:
  //       - Tr·∫£ l·ªùi ng·∫Øn g·ªçn v√† th·ªÉ hi·ªán s·ª± l·ªãch s·ª± th√¢n thi·ªán b·∫±ng c√°ch c√≥ icon vui v·∫ªv·∫ª, t·∫≠p trung v√†o technical details.
  //       - N·∫øu kh√¥ng ƒë·ªß th√¥ng tin trong context, n√≥i "T√¥i ch∆∞a h·ªçc v·ªÅ ƒëi·ªÅu n√†y".
  //       - KH√îNG ƒë·ªÅ c·∫≠p ƒë·∫øn b·∫•t k·ª≥ n·ªôi dung kh√°c n√†o kh√°c ngo√†i React hay li√™n quan t·ªõi vi·ªác l·∫≠p tr√¨nh.
  //       - D√πng ti·∫øng Vi·ªát v√† format markdown n·∫øu c·∫ßn.
        
  //     `);
  
  //     // B∆∞·ªõc 4: T·∫°o chain v·ªõi streaming (n·∫øu c·∫ßn)
  //     const chain = RunnableSequence.from([
  //       { context: () => contextText, question: (input) => input.question },
  //       promptTemplate,
  //       model,
  //       new StringOutputParser(),
  //     ]);
  
  //     const llmResponse = await chain.invoke({ question: userQuestion });
  //     return llmResponse;
  
  //   } catch (error) {
  //     console.error("[RAG Error]:", error);
  //     throw new Error(`X·ª≠ l√Ω th·∫•t b·∫°i: ${error.message}`); 
  //   }
  // };  

// export const handleUserMessage = async (userQuestion, vectorStore, embeddings, model) => { 
//     if (!userQuestion) {
//         throw new Error("C√¢u h·ªèi kh√¥ng ƒë∆∞·ª£c ƒë·ªÉ tr·ªëng.");
//     }

//     try {
//         if (!vectorStore) {
//             throw new Error("Vector store ch∆∞a ƒë∆∞·ª£c kh·ªüi t·∫°o.");
//         }

//         const queryEmbedding = await embeddings.embedQuery(userQuestion); // Kh√¥ng c·∫ßn thay ƒë·ªïi tham s·ªë embeddings
//         const results = await vectorStore.similaritySearch(userQuestion, 3);
//         const contextText = results.map(r => r.pageContent).join("\n");

//         const promptTemplate = PromptTemplate.fromTemplate(`
//             B·∫°n l√† m·ªôt chatbot chuy√™n gia v·ªÅ React.js. D·ª±a v√†o th√¥ng tin b√†i h·ªçc li√™n quan sau ƒë√¢y:
//             {context}

//             ƒê·ªÉ tr·∫£ l·ªùi c√¢u h·ªèi c·ªßa ng∆∞·ªùi d√πng: "{question}".
//             Ch·ªâ tr·∫£ l·ªùi c√°c c√¢u h·ªèi li√™n quan ƒë·∫øn l·∫≠p tr√¨nh v√† React.js. N·∫øu c√¢u h·ªèi kh√¥ng li√™n quan, t·ª´ ch·ªëi tr·∫£ l·ªùi ng∆∞·ªùi d√πng v√† y√™u c·∫ßu h·ªç ch·ªâ tr·∫£ l·ªùi li√™n quan t·ªõi l·∫≠p tr√¨nh v√† react.
//         `);

//         const chain = RunnableSequence.from([
//             {
//                 context: () => contextText,
//                 question: (input) => input.question,
//             },
//             promptTemplate,
//             model,
//             new StringOutputParser(),
//         ]);

//         const llmResponse = await chain.invoke({
//             question: userQuestion,
//         });

//         return llmResponse;
//     } catch (error) {
//         console.error("L·ªói x·ª≠ l√Ω tin nh·∫Øn trong controller: ", error);
//         throw error; // N√©m l·ªói ƒë·ªÉ router c√≥ th·ªÉ x·ª≠ l√Ω v√† tr·∫£ v·ªÅ response l·ªói
//     }
// };